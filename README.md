# LLM-Symbolic Integration for Robust Temporal Tabular Reasoning-C

This repository contains the implementation and datasets associated with the paper "LLM-Symbolic Integration for Robust Temporal Tabular Reasoning." The project addresses the challenges of temporal tabular reasoning using large language models (LLMs), focusing on structured, symbolic approaches to improve performance, robustness, and scalability.

Overview

Temporal tabular question answering requires reasoning over structured data with time-dependent attributes. Traditional LLM approaches struggle with limitations such as sensitivity to table size, reliance on memorized patterns, and reduced performance on complex queries. To address these gaps, we propose:

TEMPTABQA-C: A large-scale synthetic dataset designed for controlled and systematic evaluation of temporal reasoning tasks.

Symbolic Intermediate Representation: A structured framework where LLMs generate and execute SQL queries on database schemas, improving reasoning and mitigating biases.

Adaptive Few-Shot Prompting: Dynamically tailored examples that enhance model performance in diverse scenarios.

TEMPTABQA-C Dataset

Overview

TEMPTABQA-C is a large-scale, semi-automatically generated dataset tailored for evaluating temporal reasoning in LLMs. It overcomes limitations of traditional human-curated datasets by allowing controlled variations in data characteristics and enabling scalable generation of diverse and challenging scenarios.

Dataset Features

Controlled Evaluation: Enables systematic testing across diverse data characteristics.

Scalability: Comprises over 200,000 questions spanning a wide range of contexts and complexities.

Fine-Grained Analysis: Supports benchmarking for model biases, robustness, and limitations in temporal reasoning.

Dataset Composition and Splits

The dataset is divided into the following categories:

Original Questions: Questions derived directly from structured data.

Table Sizes:

Small Tables: Concise data representing limited records.

Large Tables: Extensive datasets with higher complexity.

Question Complexity:

Easy: Basic fact retrieval or single-step reasoning.

Medium: Multi-step reasoning, calculations, or comparisons.

Hard: Complex, multi-hop temporal reasoning.

Counterfactual Questions: Hypothetical questions that modify specific facts in the dataset to challenge robustness.

Dataset Statistics

Category

Examples

Original Questions

578

Counterfactual

699

Small Tables

855

Large Tables

538

Easy

732

Medium

507

Hard

719

Dataset Creation Pipeline

Extracting Temporal Information:

Temporal data (e.g., athletes, tournaments, achievements) was extracted from Wikipedia infoboxes.

Attributes like "Name," "Date of Birth," "Tournaments Played," and "Medals Won" were converted into a structured format.

Relational Database Construction:

The structured data was organized into a relational schema with the following key tables:

Athlete Table: Contains unique athlete_id and athlete names.

Personal Information Table: Captures details like birth year, month, and day, linked to athlete_id.

Tournament Table: Stores tournament names, linked to athlete_id.

Format Table: Represents event formats, linked to tournaments.

Medal Table: Documents medals, including type (e.g., Gold), year, and location.

Question and Answer Generation:

Questions were generated using predefined templates. Examples include:

"At what age did [Athlete] win their most recent [Tournament] [Medal Type]?"

"In which city did [Athlete] win their most recent [Tournament] medal?"

Answers were derived using SQL-based logic to query the relational database.

Counterfactual questions were generated by modifying specific attributes to create hypothetical scenarios.

SQL-Based Answer Generation:

For each question, an SQL query was generated to retrieve the correct answer from the relational database.

This approach ensures consistency, scalability, and precision in dataset generation.

Key Contributions

TEMPTABQA-C: A synthetic dataset enabling precise evaluation of temporal tabular reasoning tasks.

Symbolic Intermediate Representation: Transforming unstructured tables into database schemas for SQL-based reasoning.

Adaptive Few-Shot Prompting: Dynamically tailoring examples to improve model robustness in diverse contexts.

Experimental Setup

Evaluation Metrics

Exact Match Score (EMS): Measures the accuracy of predicted answers against ground truth.

Relaxed Exact Match Score (REMS): A relaxed version of EMS, tolerating minor variations.

Baselines and Methods

Direct Prompting: Natural language prompts with answers directly generated by LLMs.

Symbolic Intermediate Representation: LLMs generate SQL queries that are executed on the database to retrieve answers.

Few-Shot Prompting:

Zero-Shot: No examples provided.

Non-Adaptive Few-Shot: Static examples provided for all queries.

Adaptive Few-Shot: Examples dynamically selected based on query context.

Results and Findings

Robustness to Counterfactual Data:

SQL-based methods exhibited significantly smaller performance gaps compared to direct prompting.

Adaptive few-shot prompting further improved robustness.

Scalability with Table Size:

SQL methods consistently handled large tables with minimal performance degradation.

Handling Question Complexity:

SQL-based reasoning outperformed direct prompting across easy, medium, and hard questions.

Repository Structure

